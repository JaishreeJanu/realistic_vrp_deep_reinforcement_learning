{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SsWljGf-sxzK"
   },
   "outputs": [],
   "source": [
    "#!  pip install overpy  \n",
    "#!  pip install routingpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vO-k5IBFulir"
   },
   "outputs": [],
   "source": [
    "#! pip install gym\n",
    "#! pip install tianshou\n",
    "#! pip install ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pLz2fVkbvJ9p"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jatin\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:46: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "C:\\Users\\Jatin\\anaconda3\\lib\\site-packages\\numba\\core\\types\\__init__.py:108: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  long_ = _make_signed(np.long)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tianshou.data import Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jatin\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import tianshou\n",
    "from typing import Any, Callable, List, Optional, Tuple, Union, Dict\n",
    "from tianshou.data import Batch\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.data import Batch, ReplayBuffer, to_torch, to_torch_as\n",
    "from tianshou.policy import BasePolicy\n",
    "\n",
    "from tianshou.env.worker import (\n",
    "    DummyEnvWorker,\n",
    "    EnvWorker,\n",
    "    RayEnvWorker,\n",
    "    SubprocEnvWorker,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env.VRPEnv import VRPEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nets.attention_model import AttentionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AVPGZfYNs3zr",
    "outputId": "4d472c40-b7c8-467f-8ae3-8ab56f169e34"
   },
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "\n",
    "import overpy\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "from urllib import parse\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#setup OSRM \n",
    "from routingpy import OSRM\n",
    "from routingpy.routers import options\n",
    "options.default_timeout=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-lUIGI0-tVfH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jatin\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6gpyi7dqs5-J"
   },
   "outputs": [],
   "source": [
    "# For a given address query [city_name, shop_type], first we get co-ordinates and distance values from Open Street Map\n",
    "# Then generate a dataset of n_instances having graph structure information (node and edge features)\n",
    "\n",
    "# Other shop types : supermarket, convenience, clothes, hairdresser, car_repair, bakery\n",
    "\n",
    "def get_area_code(address):\n",
    "\turl = \"https://www.openstreetmap.org/geocoder/search_osm_nominatim?query=\" + parse.quote(address)\n",
    "\tr = requests.get(url) \n",
    "\tsoup = BeautifulSoup(r.content, 'html5lib')\n",
    "\tosm_link = soup.find('a', attrs = {'class':'set_position'})\n",
    "\trelation_id = osm_link.get('data-id').strip()\t\n",
    "\treturn int(relation_id) + 3600000000 # 3600000000 is the offset in ids \n",
    "\n",
    "\n",
    "\n",
    "def get_coordinates(address, shop_type):\n",
    "    # Setting up \n",
    "    area_code = get_area_code(address)\n",
    "    api = overpy.Overpass()\n",
    "\n",
    "    request = api.query(f\"\"\"area({area_code});\n",
    "    (node[shop={shop_type}](area);\n",
    "    way[shop={shop_type}](area);\n",
    "    rel[shop={shop_type}](area);\n",
    "    ); out center;\"\"\")\n",
    "\n",
    "    coords = [[float(node.lon), float(node.lat)] for node in request.nodes]\n",
    "    coords += [[float(way.center_lon), float(way.center_lat)] for way in request.ways]\n",
    "    coords += [[float(rel.center_lon), float(rel.center_lat)] for rel in request.relations]\n",
    "\n",
    "    print(f\"Total {len(coords)} points found on the map for search query: {address, shop_type}\")\n",
    "    return coords\n",
    "\n",
    "\n",
    "\n",
    "def generate_graphs(address, shop_type, graph_size, n_instances, edge_type = \"distance\", out_path = \"./cvrp_data/test.pk\"):\n",
    "    coordinates = get_coordinates(address, shop_type)\n",
    "    \n",
    "    client = OSRM(base_url=\"https://router.project-osrm.org\")\n",
    "    instances = []\n",
    "\n",
    "    for _ in range (n_instances):\n",
    "        instance_coords = random.sample(coordinates, graph_size) # Node coordinates for the graph are obtained by randomly sampling graph_size nodes from all queried locations on OSM\n",
    "        dist_matrix = client.matrix(locations=instance_coords, profile=\"car\")\n",
    "\n",
    "        if edge_type == \"distance\":\n",
    "            edge_features = np.array(dist_matrix.distances)\n",
    "        elif edge_type == \"time\":\n",
    "            edge_features = np.array(dist_matrix.durations)\n",
    "            \n",
    "        edge_features = torch.from_numpy(edge_features).float()\n",
    "        instance = {\"coordinates\": instance_coords, \"edge_features\": edge_features}\n",
    "        instances.append(instance)\n",
    "\n",
    "\n",
    "    # Now generating node features for all instances -- assigning first node as depot with no demand, and all others with equal demands (for now)\n",
    "    # Node features: [is_depot, is_customer, demand, coordinate_1, coordinate_2]\n",
    "    all_graphs = []\n",
    "    demand = 1/(len(instances[0][\"coordinates\"])-1) #Equal demand for all customers\n",
    "    for instance in instances:\n",
    "        graph_nodes = []\n",
    "        coordinates = instance[\"coordinates\"]\n",
    "\n",
    "        for idx, node_coordinates in enumerate(coordinates):\n",
    "\n",
    "            if idx == 0:\n",
    "                node_features = np.append(np.array([1, 0, 0]), node_coordinates)\n",
    "                graph_nodes = node_features\n",
    "            else:\n",
    "                node_features = np.append(np.array([0, 1, demand]), node_coordinates)\n",
    "                graph_nodes = np.vstack((graph_nodes, node_features))\n",
    "                \n",
    "        graph_nodes = torch.from_numpy(graph_nodes).float()\n",
    "        graph_struct = {\"node_features\": graph_nodes, \"edge_features\": instance[\"edge_features\"], \"coordinates\": coordinates}\n",
    "        all_graphs.append(graph_struct)\n",
    "\n",
    "    return all_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0uScdlul2hYe",
    "outputId": "01d29453-d002-483e-b652-c69b78c1f62e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 863 points found on the map for search query: ('Berlin', 'supermarket')\n"
     ]
    }
   ],
   "source": [
    "graphs = generate_graphs(address=\"Berlin\", shop_type=\"supermarket\", graph_size=10, n_instances=5)\n",
    "#print(graphs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "HIx7x68lugCJ"
   },
   "outputs": [],
   "source": [
    "class VRPDataset(Dataset):  \n",
    "        \n",
    "    def __init__(self, graph_size=10, n_instances=5, address=\"Berlin\", shop_type=\"supermarket\"):\n",
    "      #super().__init__()\n",
    "      self.vrpdataset = generate_graphs(address, shop_type, graph_size, n_instances)\n",
    "    \n",
    "    def __len__(self):\n",
    "      return len(self.vrpdataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.vrpdataset[index]\n",
    "        return data\n",
    "\n",
    "    def sample_graphs_viz(self, n_sample=5):\n",
    "        sampled_graphs = random.sample(self.vrpdataset, n_sample)\n",
    "        \n",
    "        # define subplot grid\n",
    "        fig, axs = plt.subplots(nrows=n_sample, ncols=1, figsize=(6,20))\n",
    "        plt.subplots_adjust(hspace=0.5)\n",
    "        fig.suptitle(f\"Sampled {n_sample} graphs from the dataset\", fontsize=16, y=0.95)\n",
    "        \n",
    "        # loop through graphs and axes\n",
    "        s_idx = 0\n",
    "        labels = [f\"c_{i}\" for i in range(len(sampled_graphs[0][\"coordinates\"]))]\n",
    "        labels[0] = \"depot\"\n",
    "        for graph, ax in zip(sampled_graphs, axs.ravel()):\n",
    "            longitudes = [coord[0] for coord in graph[\"coordinates\"]]\n",
    "            latitudes = [coord[1] for coord in graph[\"coordinates\"]]\n",
    "            # filter df for ticker and plot on specified axes\n",
    "            ax.scatter(longitudes, latitudes)\n",
    "\n",
    "            # chart formatting\n",
    "            s_idx += 1\n",
    "            ax.set_title(f\"Sample {s_idx}\")\n",
    "            ax.set_xlabel(\"longitudes\")\n",
    "            ax.set_ylabel(\"latitudes\")\n",
    "\n",
    "            for i, txt in enumerate(labels):\n",
    "                ax.annotate(txt, (longitudes[i], latitudes[i]))\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mmPbLRG3Lv4L",
    "outputId": "c3cd4707-441d-4d6e-a0a0-2e3961814c21"
   },
   "outputs": [],
   "source": [
    "#dataset = VRPDataset(graph_size=10, n_instances=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_fns = [lambda instance=graph, idx=i: VRPEnv(instance, idx) for i,graph in enumerate(graphs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_envs = DummyVectorEnv(env_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_envs = DummyVectorEnv(env_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttentionModel(\n",
    "        embedding_dim=64,\n",
    "        hidden_dim=16,\n",
    "        n_encode_layers=2,\n",
    "        tanh_clipping=10.,\n",
    "        mask_inner=True, #to be fixed\n",
    "        mask_logits=True,\n",
    "        normalization='batch',\n",
    "        n_heads=8,\n",
    "        checkpoint_encoder=False,\n",
    "        shrink_size=None\n",
    "    )\n",
    "\n",
    "#print(model)\n",
    "\n",
    "##print(obs['demand'][obs['ids'], :] + obs['used_capacity'][0][:, :, None] > obs['capacity'] )\n",
    "#action = model.forward(obs)\n",
    "#print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCEPolicy(BasePolicy):\n",
    "    \"\"\"Implementation of REINFORCE algorithm.\"\"\"\n",
    "    def __init__(self, model: torch.nn.Module, optim: torch.optim.Optimizer,):\n",
    "        super().__init__()\n",
    "        self.actor = model\n",
    "        self.optim = optim\n",
    "        # action distribution\n",
    "        self.dist_fn = torch.distributions.Categorical\n",
    "\n",
    "        \n",
    "    def forward(self, batch: Batch) -> Batch:\n",
    "        \"\"\"Compute action over the given batch data.\"\"\"\n",
    "        logits = self.actor(batch)\n",
    "        #print(f\"logits: {logits}\")\n",
    "        dist = self.dist_fn(logits=logits)\n",
    "        #print(dist.probs)\n",
    "        act = self.get_actions(dist, batch)\n",
    "        #print(batch[\"action_mask\"])\n",
    "        #print(batch[\"remaining_capacity\"])\n",
    "        #print(act)\n",
    "        return Batch(act=act, dist=dist)\n",
    "\n",
    "\n",
    "    def process_fn(self, batch: Batch, buffer: ReplayBuffer, indices: np.ndarray) -> Batch:\n",
    "        \"\"\"Compute the discounted returns for each transition.\"\"\"\n",
    "        returns, _ = self.compute_episodic_return(batch, buffer, indices, gamma=0.99, gae_lambda=1.0)\n",
    "        batch.returns = returns\n",
    "        print(f\"pre-batch: {batch}\")\n",
    "        return batch\n",
    "\n",
    "\n",
    "    def learn(self, batch: Batch, batch_size: int, repeat: int) -> Dict[str, List[float]]:\n",
    "        \"\"\"Perform the back-propagation.\"\"\"\n",
    "        logging_losses = []\n",
    "        for _ in range(repeat):\n",
    "            for minibatch in batch.split(batch_size, merge_last=True):\n",
    "                self.optim.zero_grad()\n",
    "                result = self(minibatch)\n",
    "                dist = result.dist\n",
    "                act = to_torch_as(minibatch.act, result.act)\n",
    "                ret = to_torch(minibatch.returns, torch.float, result.act.device)\n",
    "                log_prob = dist.log_prob(act).reshape(len(ret), -1).transpose(0, 1)\n",
    "                loss = -(log_prob * ret).mean()\n",
    "                loss.backward()\n",
    "                self.optim.step()\n",
    "                logging_losses.append(loss.item())\n",
    "        return {\"loss\": logging_losses}\n",
    "    \n",
    "    \n",
    "    def get_actions(self, dist, batch_obs):\n",
    "        actions = []\n",
    "        for idx, (probs, mask) in enumerate(zip(dist.probs, batch_obs[\"action_mask\"])):\n",
    "            demands = batch_obs[\"node_features\"][idx,:,2]\n",
    "            mask_tensor = ~torch.tensor(mask, dtype=torch.bool)\n",
    "            \n",
    "            action_order = np.array(torch.topk(probs, 3).indices)\n",
    "            \n",
    "            #print(probs)\n",
    "            #print(action_order)\n",
    "            \n",
    "            n = int(action_order.shape[1])\n",
    "            \n",
    "            action = int(action_order[0,0])\n",
    "            \n",
    "            if action == 0: #If selected node is depot node, confirm if any step is possible to next probable node\n",
    "                alt_action = int(action_order[0,1])\n",
    "                if demands[alt_action] <= batch_obs[\"remaining_capacity\"][idx]:\n",
    "                    action = alt_action\n",
    "                    \n",
    "            if batch_obs[\"remaining_capacity\"][idx] < demands[action]:\n",
    "                action = 0\n",
    "                    \n",
    "            #print(action)\n",
    "            actions.append(action)\n",
    "        actions = torch.tensor(actions).view(-1, 1)\n",
    "        return actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jatin\\anaconda3\\lib\\site-packages\\numba\\np\\arraymath.py:3806: DeprecationWarning: `np.MachAr` is deprecated (NumPy 1.22).\n",
      "  @overload(np.MachAr)\n",
      "C:\\Users\\Jatin\\anaconda3\\lib\\site-packages\\numba\\core\\ir_utils.py:1525: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if (hasattr(numpy, value)\n",
      "C:\\Users\\Jatin\\anaconda3\\lib\\site-packages\\numba\\core\\ir_utils.py:1525: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if (hasattr(numpy, value)\n",
      "C:\\Users\\Jatin\\anaconda3\\lib\\site-packages\\numba\\core\\ir_utils.py:1525: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if (hasattr(numpy, value)\n",
      "C:\\Users\\Jatin\\anaconda3\\lib\\site-packages\\numba\\core\\ir_utils.py:1525: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if (hasattr(numpy, value)\n"
     ]
    }
   ],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
    "VRPpolicy = REINFORCEPolicy(model, optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jatin\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "#print(VRPpolicy)\n",
    "#print(\"========================================\")\n",
    "#for para in VRPpolicy.parameters():\n",
    "    #print(para.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env: 0 reset\n",
      "Env: 1 reset\n",
      "Env: 2 reset\n",
      "Env: 3 reset\n",
      "Env: 4 reset\n"
     ]
    }
   ],
   "source": [
    "init_obs = Batch(test_envs.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'obs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-70b5ba31c76f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVRPpolicy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_obs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-78997d0b5101>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mBatch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mBatch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;34m\"\"\"Compute action over the given batch data.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;31m#print(f\"logits: {logits}\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdist_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\Python_Bin\\Project SRP\\My Codes\\RealisticVRP-dev\\nets\\attention_model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    120\u001b[0m         \"\"\"\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"obs\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m             \u001b[0mbatch_obs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"obs\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tianshou\\data\\batch.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[1;34m\"\"\"Return self[index].\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m         \u001b[0mbatch_items\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_items\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'obs'"
     ]
    }
   ],
   "source": [
    "action = VRPpolicy(init_obs)\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_envs.reset()\n",
    "#result = test_envs.step(action.act)\n",
    "#obs_next, rew, done, info = result\n",
    "#obs_next = Batch(obs_next)\n",
    "#\n",
    "#print(obs_next[\"action_mask\"])\n",
    "#print(obs_next[\"curr_pos_idx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_envs.reset()\n",
    "obs_next = init_obs\n",
    "for _ in range(10):\n",
    "    print(\"\\n------------------------------------------------\")\n",
    "    action2 = VRPpolicy(obs_next)\n",
    "    result2 = test_envs.step(action2.act)\n",
    "    obs_next, rew, done, info = result2\n",
    "    obs_next = Batch(obs_next)\n",
    "    print(obs_next[\"action_mask\"])\n",
    "    #print(obs_next[\"curr_pos_idx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obs_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.VRPCollector import Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tianshou.data import VectorReplayBuffer\n",
    "buffer_size = 100\n",
    "replaybuffer = VectorReplayBuffer(buffer_size, buffer_num=5)\n",
    "\n",
    "\n",
    "#test_collector = Collector(VRPpolicy, test_envs, replaybuffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_collector = Collector(VRPpolicy, test_envs)\n",
    "train_collector = Collector(VRPpolicy, train_envs, replaybuffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(replaybuffer[\"obs_next\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_result = test_collector.collect(n_episode=10)\n",
    "print(collect_result)\n",
    "print(\"Rewards of all environments are {}\".format(collect_result[\"rews\"]))\n",
    "print(\"Average episode reward is {}.\".format(collect_result[\"rew\"]))\n",
    "print(\"Average episode length is {}.\".format(collect_result[\"len\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replaybuffer.sample(9)\n",
    "\n",
    "#test_collector = Collector(VRPpolicy, test_envs, replaybuffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tianshou.trainer import onpolicy_trainer\n",
    "\n",
    "train_collector.reset()\n",
    "train_envs.reset()\n",
    "test_collector.reset()\n",
    "test_envs.reset()\n",
    "replaybuffer.reset()\n",
    "\n",
    "result = onpolicy_trainer(\n",
    "    VRPpolicy,\n",
    "    train_collector,\n",
    "    test_collector,\n",
    "    max_epoch=10,\n",
    "    step_per_epoch=10,\n",
    "    repeat_per_collect=1,\n",
    "    episode_per_test=1,\n",
    "    episode_per_collect=10,\n",
    "    batch_size=5,\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
